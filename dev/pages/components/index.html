<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Components · Susi</title><link rel="canonical" href="https://andreasdominik.github.io/Susi/dev/pages/components/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="Susi logo"/></a><div class="docs-package-name"><span class="docs-autofit">Susi</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Introduction</a></li><li><a class="tocitem" href="../install/">Installation</a></li><li><a class="tocitem" href="../configuration/">Configuration</a></li><li class="is-active"><a class="tocitem" href>Components</a><ul class="internal"><li><a class="tocitem" href="#Hotword-detection-1"><span>Hotword detection</span></a></li><li><a class="tocitem" href="#Record-1"><span>Record</span></a></li><li><a class="tocitem" href="#Speech-to-text-(STT)-1"><span>Speech to text (STT)</span></a></li><li><a class="tocitem" href="#Text-to-speech-(TTS)-1"><span>Text to speech (TTS)</span></a></li><li><a class="tocitem" href="#Play-1"><span>Play</span></a></li><li><a class="tocitem" href="#Session-manager-1"><span>Session manager</span></a></li><li><a class="tocitem" href="#NLU-natural-language-understanding-1"><span>NLU - natural language understanding</span></a></li></ul></li><li><a class="tocitem" href="../nlu/">NLU</a></li><li><a class="tocitem" href="../topics/">Topics</a></li><li><a class="tocitem" href="../payloads/">Payloads</a></li><li><a class="tocitem" href="../tools/">Tools</a></li><li><a class="tocitem" href="../../LICENSE/">License</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Components</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Components</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/andreasdominik/Susi/blob/master/docs/src/pages/components.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Daemons-and-components-1"><a class="docs-heading-anchor" href="#Daemons-and-components-1">Daemons and components</a><a class="docs-heading-anchor-permalink" href="#Daemons-and-components-1" title="Permalink"></a></h1><h2 id="Hotword-detection-1"><a class="docs-heading-anchor" href="#Hotword-detection-1">Hotword detection</a><a class="docs-heading-anchor-permalink" href="#Hotword-detection-1" title="Permalink"></a></h2><p>By default, Snowboy is used as hotword detector, because of its high accuracy.</p><h2 id="Record-1"><a class="docs-heading-anchor" href="#Record-1">Record</a><a class="docs-heading-anchor-permalink" href="#Record-1" title="Permalink"></a></h2><p>The Swiss Army knife of sound processing programs (SoX - Sound eXchange) is used as default recording service. The following rec-command records audio with the configuration:</p><ul><li>maximum lenght TIME_MAX seconds</li><li>wait with recording until a sound is detected (no silence)</li><li>stop recording if 1 second of silence is recorded</li><li>remix to one channel</li><li>use a bitrate of 16000</li><li>add 10 dB of gain</li></ul><pre><code class="language-none">rec --rate 16000 $AUDIO \
    trim 0 $TIME_MAX \
    silence 1 0:00.05 $NOISE 1 $END_TRIGGER $NOISE \
    remix 1-2 \
    gain 10</code></pre><h2 id="Speech-to-text-(STT)-1"><a class="docs-heading-anchor" href="#Speech-to-text-(STT)-1">Speech to text (STT)</a><a class="docs-heading-anchor-permalink" href="#Speech-to-text-(STT)-1" title="Permalink"></a></h2><p>Speech recognition is the cruical part of every assistant. Unfortunately not very much open software with the desired quality is available currently.</p><p>There are a number of potentially usable open software available, such as</p><ul><li><strong>Mozilla DeepSpeech</strong> (https://github.com/mozilla/DeepSpeech), for which a binary is already included in the distribution.</li><li><strong>Kaldi</strong> (http://kaldi-asr.org/) provides already some high-quality model for different languages</li><li>For Facebook&#39;s <strong>wav2letter</strong> (https://github.com/facebookresearch/wav2letter) trained models are available, too.</li><li>High-quality trained models and tools are also provided by <strong>Zamia AI</strong> (http://zamia-speech.org/asr/).</li></ul><p>In contrast, commercial web-services with almost perfect quality are availble, such as:</p><ul><li>Google Cloud Speech-to-Text (https://cloud.google.com/speech-to-text/),</li><li>Wit.ai (https://wit.ai),</li><li>Google Dialogue Flow (https://dialogflow.com/),</li><li>Microsoft Cognitive Services (https://azure.microsoft.com/en-us/services/cognitive-services),</li><li>Watson Speech to Text (https://www.ibm.com/cloud/watson-speech-to-text),</li><li>Speechmatics (https://www.speechmatics.com/),</li><li>Amazon (https://aws.amazon.com/transcribe/)</li><li>etc.</li></ul><p>All of them require sending the recorded audio to the server somewhere in the internet, which breaks the privacy we are used to with Snips.ai.</p><p>However, speech recognition technology develops very fast and using a cloud service may be necessary for a period of transition, until locally installed and <em>open</em> software with compareble is available.</p><h3 id="Google-Cloud-STT-1"><a class="docs-heading-anchor" href="#Google-Cloud-STT-1">Google Cloud STT</a><a class="docs-heading-anchor-permalink" href="#Google-Cloud-STT-1" title="Permalink"></a></h3><p>The Google Cloud connector is configured by default in Susi, because of the very high quality of the transcriptions. The service ist sensitive, accurate available for many languages and has a good common knowlegde (knows names of famous persons, titles of movies and TV shows, etc.).</p><h4 id="Configuration-of-Google-Cloud-services:-1"><a class="docs-heading-anchor" href="#Configuration-of-Google-Cloud-services:-1">Configuration of Google Cloud services:</a><a class="docs-heading-anchor-permalink" href="#Configuration-of-Google-Cloud-services:-1" title="Permalink"></a></h4><p>If google services are used for text-to-speech (TTS) or speech-to-text (STT) the required softwate must be set up: Go through Google&#39;s tutorial <a href="https://cloud.google.com/text-to-speech/docs/quickstart-protocol">Quickstart: Using the command line</a>.</p><p>In summary ...</p><ul><li>a Google Cloud Platform Project is needed,</li><li>the Cloud Text-to-Speech API must be enabled and</li><li>the JSON-file with the credentials must be downloaded to <code>/opt/Susi/ApplicationData/Google/Credentials/google-credentials.json</code>     Path and filename may differ - they are specified in the susi configuration file.</li><li>the path to the credentials file must be made available by an variable. Edit the file <code>.bashrc</code> in the home directory of the user who will later run the assistent (e.g. <code>susi</code>) and add the line:    </li></ul><p><code>export GOOGLE_APPLICATION_CREDENTIALS=&quot;/opt/Susi/ApplicationData/Google/Credentials/google-credentials.json&quot;</code>    </p><ul><li>gcloud API must be locally installed (see the quickstart documentation):</li></ul><pre><code class="language-none">sudo echo &quot;deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main&quot; | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
sudo apt-get install apt-transport-https ca-certificates gnupg
curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
sudo apt-get update &amp;&amp; sudo apt-get install google-cloud-sdk</code></pre><p>To check the installation run the following command.   It should print an access token, which can be uses to access the Cloud   Text-to-Speech API:</p><pre><code class="language-none">gcloud auth application-default print-access-token</code></pre><h3 id="IBM-Cloud-services-1"><a class="docs-heading-anchor" href="#IBM-Cloud-services-1">IBM Cloud services</a><a class="docs-heading-anchor-permalink" href="#IBM-Cloud-services-1" title="Permalink"></a></h3><p>Transcription quality  of IBM Cloud STT is sufficent for an assistant. (At least) in Europe the latency is smaller compared to the Google service (approx. half).</p><h4 id="Configuration-of-IBM-Cloud-services-1"><a class="docs-heading-anchor" href="#Configuration-of-IBM-Cloud-services-1">Configuration of IBM Cloud services</a><a class="docs-heading-anchor-permalink" href="#Configuration-of-IBM-Cloud-services-1" title="Permalink"></a></h4><p>To use IBM Watson Text to Speech STT or TTS, it must be configured as described on IBM&#39;s website: https://cloud.ibm.com/.</p><p>Is is as simple as:</p><ul><li>create an account</li><li>descide for a pricing plan (the &quot;Free Lite Plan&quot; may be sufficient as it offers up to 500 minutes of audio transcription and 10000 characters for TTS per month)</li><li>create a Speech to Text Service (and a Text to Sppech Service)</li><li>download the credential file <code>ibm-cedentials.env</code>, rename and save it at <code>/opt/Susi/ApplicationData/IBMCloud/ibm-tts-cedentials.env</code> (the download link is in the   &#39;Manage&#39; section)</li><li>work through the &quot;Getting started with Sppech to Text&quot; tutorial (https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-gettingStarted#getting-started-tutorial).</li></ul><p>Similar steps are necessary for text to speech. Please notice, that there are different credentials necessary for STT and TTS. Both need to be downloaded and saved to <code>/opt/Susi/ApplicationData/IBMCloud/</code> with different names (such as &#39;ibm-tts-cedentials.env&#39; and &#39;ibm-stt-cedentials.env&#39;). Names can be configured in the &#39;susi.toml&#39; file.</p><h3 id="Mozilla-DeepSpeech-1"><a class="docs-heading-anchor" href="#Mozilla-DeepSpeech-1">Mozilla DeepSpeech</a><a class="docs-heading-anchor-permalink" href="#Mozilla-DeepSpeech-1" title="Permalink"></a></h3><p>As an alternative to the Google Cloud services Mozilla DeepSpeech can be used. However,</p><ul><li>trained models and language models are only available for English langage</li><li>the quality of transcription seems not to be sufficient for an assistant (at least in my tests - this may differ for other speakers and different hardware)</li><li>a transcription needs 2-5 seconds.</li></ul><p>However, it is easily possible to set up a separate STT-server with sufficient CPU power (and maybe a GPU) and integrate it to Susi.</p><h4 id="Installation-of-Mozilla-DeepSpeech-1"><a class="docs-heading-anchor" href="#Installation-of-Mozilla-DeepSpeech-1">Installation of Mozilla DeepSpeech</a><a class="docs-heading-anchor-permalink" href="#Installation-of-Mozilla-DeepSpeech-1" title="Permalink"></a></h4><p>Installation is simple and follows the instruction on the website (https://github.com/mozilla/DeepSpeech). The installation can be tested by running deepspeech on the commandline.</p><p>DeepSpeech integration to Susi is already included in the distribution and can be activated by uncommenting the line in the configuration file.</p><pre><code class="language-none"># installation of Mozilla DeepSpeech:
#
# prepare:
mkdir /opt/DeepSpeech
cd /opt/DeepSpeech
virtualenv -p python3 ./deepspeech-venv/
source $HOME/tmp/deepspeech-venv/bin/activate

# Install DeepSpeech
pip3 install deepspeech

# Download pre-trained English model and extract
curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.6.1/deepspeech-0.6.1-models.tar.gz
tar xvf deepspeech-0.6.1-models.tar.gz

# Download example audio files
curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.6.1/audio-0.6.1.tar.gz
tar xvf audio-0.6.1.tar.gz

# Transcribe an audio file
rec -r 16000 lighton.wav

deepspeech --model deepspeech-0.6.1-models/output_graph.pbmm --lm deepspeech-0.6.1-models/lm.binary --trie deepspeech-0.6.1-models/trie --audio lighton.wav</code></pre><h3 id="Snips-ASR-1"><a class="docs-heading-anchor" href="#Snips-ASR-1">Snips ASR</a><a class="docs-heading-anchor-permalink" href="#Snips-ASR-1" title="Permalink"></a></h3><p>The ASR component of Snips takes its power from the integration with NLU; i.e. the ASR knows all sentences and phrases that must be recognised and hence the language model can be adapted to each individual assistant. Because the Snips Console (the Web-interface to Snips ASR and NLU) is no longer open for the public, it is no longer possible to train the speech recognition for new skills.</p><p>Two potential ways to use Snips ASR still remain:</p><ul><li>A <strong>customised trained existing model</strong> can be used, as long as no new Skills and intents are added to an assistant, that hase been downloaded from the Snips console.</li><li>Snips provides a pretrained <strong>general model for English language</strong> that can be used for any intent. However, the model is huge, compared to the customised models, and therefore transcription needs several seconds on a Raspberry pi. To use the model, it is recommended to install the STT component on a more powerful headless server (such as a NUC or a discarded laptop).</li></ul><p>If a trained model is available and Snips ASR is installed, it can be activated by selecting the respective binary in the STT section of the configuration file &#39;susi.toml.</p><h4 id="Installation-and-configuration-od-Snips-ASR-1"><a class="docs-heading-anchor" href="#Installation-and-configuration-od-Snips-ASR-1">Installation and configuration od Snips ASR</a><a class="docs-heading-anchor-permalink" href="#Installation-and-configuration-od-Snips-ASR-1" title="Permalink"></a></h4><p>To install the Snips asr</p><ul><li>add the Snips apt-get repository,</li><li>install snips-asr,</li><li>make sure that the service is not runing (Susi will start asr when it needs it),</li><li>if you have no personally trained model, there is a general model (English language only) that can be used:</li></ul><pre><code class="language-none">sudo bash -c  &#39;echo &quot;deb https://debian.snips.ai/stretch stable main&quot; &gt; /etc/apt/sources.list.d/snips.list&#39;
sudo apt-key adv --fetch-keys  https://debian.snips.ai/5FFCD0DEB5BA45CD.pub
sudo apt-get update
sudo apt-get install snips-asr
sudo systemctl stop snips-asr
sudo systemctl disable snips-asr

# the general model:
# sudo apt-get install snips-asr-model-en-500mb</code></pre><p>A version of the general model is incuded inthe Susi distribution and strored at &#39;/opt/Susi/ApplicationData/Snips/ASRmodels&#39;. To use it, just unpack it:</p><pre><code class="language-none">cd /opt/Susi/ApplicationData/Snips/ASRmodels
tar xvf snips-asr-model-en-500MB.tar.gz</code></pre><p>In the STT section of the configuration file &#39;susi.toml&#39; the binary must be set to <code>snips-asr</code> (just uncomment the respective line) and to use the general model, the model path must point to the directory to which the model was saved.</p><h2 id="Text-to-speech-(TTS)-1"><a class="docs-heading-anchor" href="#Text-to-speech-(TTS)-1">Text to speech (TTS)</a><a class="docs-heading-anchor-permalink" href="#Text-to-speech-(TTS)-1" title="Permalink"></a></h2><h4 id="Google-Cloud-TTS-1"><a class="docs-heading-anchor" href="#Google-Cloud-TTS-1">Google Cloud TTS</a><a class="docs-heading-anchor-permalink" href="#Google-Cloud-TTS-1" title="Permalink"></a></h4><p>Google&#39;s text to speech service is used as default service, because it provides the most realistic voices. In order to reduce calls to the Google Cloud, all retrieved audio will be cached and reused if the same sentence is needed again.</p><p>Available voices can be tested here: https://cloud.google.com/text-to-speech.</p><h4 id="Mozilla-1"><a class="docs-heading-anchor" href="#Mozilla-1">Mozilla</a><a class="docs-heading-anchor-permalink" href="#Mozilla-1" title="Permalink"></a></h4><p>Mozilla&#39;s Deep Voice is not yet implemented.</p><h4 id="IBM-Cloud-TTS-1"><a class="docs-heading-anchor" href="#IBM-Cloud-TTS-1">IBM Cloud TTS</a><a class="docs-heading-anchor-permalink" href="#IBM-Cloud-TTS-1" title="Permalink"></a></h4><p>The connector to the IBM Clouds text-to-speech service is alrady included in the distribution. It can be selected by uncommenting the respective line for the binary in the <code>[tts]</code>-section of the configuration file &#39;susi.toml&#39;.</p><p>Please notice, that there are different credentials for STT and TTS.</p><p>Example voices can be listened to at: https://www.ibm.com/de-de/cloud/watson-text-to-speech and https://cloud.ibm.com/docs/text-to-speech?topic=text-to-speech-voices.</p><h2 id="Play-1"><a class="docs-heading-anchor" href="#Play-1">Play</a><a class="docs-heading-anchor-permalink" href="#Play-1" title="Permalink"></a></h2><p>Susi&#39;s play component uses linux sox play (SoX - Sound eXchange). The play.daemon subscribes to the topic &#39;susi/playserver/request&#39;with a payload that include a base64-encoded audio file and the siteId on which the audio must be played.</p><p>The daemon can play all audio formats that sox play is able to play (for mp3 &#39;libsox-fmt-mp3&#39; must be installed).</p><p>The length of the audio is not limited.</p><p>It is recommended not to send requests directly to the play daemon, but use the session manager API instead (topic: &#39;susi/play/playAudio&#39;). This ensures that timeouts are postponed during playing.</p><h2 id="Session-manager-1"><a class="docs-heading-anchor" href="#Session-manager-1">Session manager</a><a class="docs-heading-anchor-permalink" href="#Session-manager-1" title="Permalink"></a></h2><p>The session manager runs sequences of actions, such as <code>hotword -&gt; record -&gt; stt -&gt; nlu -&gt; publish intent</code> and takes care of timeouts and non-responding components.</p><h2 id="NLU-natural-language-understanding-1"><a class="docs-heading-anchor" href="#NLU-natural-language-understanding-1">NLU - natural language understanding</a><a class="docs-heading-anchor-permalink" href="#NLU-natural-language-understanding-1" title="Permalink"></a></h2><p>Please refer to the NLU-Section of the documentation for more details.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../configuration/">« Configuration</a><a class="docs-footer-nextpage" href="../nlu/">NLU »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 21 April 2020 14:10">Tuesday 21 April 2020</span>. Using Julia version 1.2.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
